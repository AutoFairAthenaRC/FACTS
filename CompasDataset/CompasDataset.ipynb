{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ProPublica Recidivism/COMPAS` :<br>\n",
    "    The COMPAS dataset (Correctional Offender Management Profiling for Alternative Sanctions) is a dataset used for binary classification tasks in the field of criminal justice. The goal is to predict whether a defendant will re-offend within two years of release, based on various demographic and criminal history features.<br>\n",
    "    Reference Link: https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import facts\n",
    "from facts.clean import clean_dataset\n",
    "from facts import valid_ifthens_with_coverage_correctness, rules2rulesbyif\n",
    "from facts.models import customLogisticRegression\n",
    "from facts.parameters import ParameterProxy\n",
    "from facts.formatting import recourse_report_reverse, print_recourse_report, print_recourse_report_cumulative\n",
    "import matplotlib.pyplot as plt\n",
    "from aif360.sklearn.datasets import fetch_compas\n",
    "\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>age_cat</th>\n",
       "      <th>race</th>\n",
       "      <th>juv_fel_count</th>\n",
       "      <th>juv_misd_count</th>\n",
       "      <th>juv_other_count</th>\n",
       "      <th>priors_count</th>\n",
       "      <th>c_charge_degree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Male</td>\n",
       "      <td>25 - 45</td>\n",
       "      <td>African-American</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(-0.1, 1.0]</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Male</td>\n",
       "      <td>10-25</td>\n",
       "      <td>African-American</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>(1.0, 5.0]</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Male</td>\n",
       "      <td>25 - 45</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(10.0, 15.0]</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Female</td>\n",
       "      <td>25 - 45</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(-0.1, 1.0]</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Male</td>\n",
       "      <td>25 - 45</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(-0.1, 1.0]</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sex  age_cat              race  juv_fel_count  juv_misd_count  \\\n",
       "0    Male  25 - 45  African-American              0               0   \n",
       "1    Male    10-25  African-American              0               0   \n",
       "2    Male  25 - 45         Caucasian              0               0   \n",
       "3  Female  25 - 45         Caucasian              0               0   \n",
       "4    Male  25 - 45         Caucasian              0               0   \n",
       "\n",
       "   juv_other_count  priors_count c_charge_degree  \n",
       "0                0   (-0.1, 1.0]               F  \n",
       "1                1    (1.0, 5.0]               F  \n",
       "2                0  (10.0, 15.0]               F  \n",
       "3                0   (-0.1, 1.0]               M  \n",
       "4                0   (-0.1, 1.0]               F  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = fetch_compas()\n",
    "X['target'] = y.values\n",
    "\n",
    "sensitive_attribute = \"race\"\n",
    "df = clean_dataset(X,'compas')\n",
    "y = df['target']\n",
    "X = df.drop('target', axis=1)\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_features = X._get_numeric_data().columns.to_list()\n",
    "cate_features = X.select_dtypes(include=['object','category']).columns.to_list()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=None, stratify=y)\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 109 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
       "       1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
       "       1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0.,\n",
       "       0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0.,\n",
       "       1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = customLogisticRegression(max_iter = 1500)\n",
    "model.fit(X_train, y_train, cate_columns=cate_features, target_column='target')\n",
    "model.predict(X_test.iloc[:100, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.63      0.63       745\n",
      "           1       0.67      0.68      0.67       837\n",
      "\n",
      "    accuracy                           0.65      1582\n",
      "   macro avg       0.65      0.65      0.65      1582\n",
      "weighted avg       0.65      0.65      0.65      1582\n",
      "\n"
     ]
    }
   ],
   "source": [
    "positive_label = 1\n",
    "negative_label = 0\n",
    "preds = model.predict(X_test)\n",
    "print(classification_report(y_test.map({negative_label: 0, positive_label: 1}), preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conf_matrix = confusion_matrix(y_test, preds)\n",
    "cm_disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)\n",
    "cm_disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find all valid if-thens with all respective coverages and correctness, for all subgroups.\n",
    "\n",
    "**Caution!** This step takes time. Uncomment the following block if you wish to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing frequent itemsets for each subgroup of the affected instances.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 12.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the intersection between the frequent itemsets of each subgroup of the affected instances.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1392/1392 [00:00<00:00, 199469.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing all valid if-then pairs between the common frequent itemsets of each subgroup of the affected instances and the frequent itemsets of the unaffacted instances.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1050/1050 [00:00<00:00, 9817.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing correctenesses for all valid if-thens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 3831/3831 [00:57<00:00, 66.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of if-thens: 3831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Uncomment if you want to find new rules with different support\n",
    "\n",
    "ifthens_coverage_correctness = valid_ifthens_with_coverage_correctness(\n",
    "    X_test,\n",
    "    model=model,\n",
    "    sensitive_attribute='race',\n",
    "    freqitem_minsupp = 0.01,\n",
    "    missing_subgroup_val=\"Unknown\",\n",
    "    drop_infeasible = True,\n",
    "    drop_above = False\n",
    ")\n",
    "\n",
    "print(f\"Number of if-thens: {len(ifthens_coverage_correctness)}\")\n",
    "rules_by_if = rules2rulesbyif(ifthens_coverage_correctness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from facts.utils import load_rules_by_if,save_rules_by_if\n",
    "# save_rules_by_if(\"rulesCompas_atomic.data\", rules_by_if)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #### Uncomment if you wish to load rules object from disk\n",
    "\n",
    "# import dill\n",
    "\n",
    "# with open(\"rulesCompas_atomic.data\", \"rb\") as inf:\n",
    "#     rules_by_if = dill.load(inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If \u001b[1mjuv_other_count = 3\u001b[0m:\n",
      "\tProtected Subgroup '\u001b[1mAfrican-American\u001b[0m', \u001b[34m1.23%\u001b[39m covered\n",
      "\t\tMake \u001b[1m\u001b[31mjuv_other_count = 0\u001b[39m\u001b[0m with correctness \u001b[32m0.00%\u001b[39m.\n",
      "\tProtected Subgroup '\u001b[1mCaucasian\u001b[0m', \u001b[34m2.38%\u001b[39m covered\n",
      "\t\tMake \u001b[1m\u001b[31mjuv_other_count = 0\u001b[39m\u001b[0m with correctness \u001b[32m100.00%\u001b[39m.\n",
      "If \u001b[1mjuv_fel_count = 0, juv_other_count = 3\u001b[0m:\n",
      "\tProtected Subgroup '\u001b[1mAfrican-American\u001b[0m', \u001b[34m1.23%\u001b[39m covered\n",
      "\t\tMake \u001b[1m\u001b[31mjuv_other_count = 0\u001b[39m\u001b[0m with correctness \u001b[32m0.00%\u001b[39m.\n",
      "\tProtected Subgroup '\u001b[1mCaucasian\u001b[0m', \u001b[34m2.38%\u001b[39m covered\n",
      "\t\tMake \u001b[1m\u001b[31mjuv_other_count = 0\u001b[39m\u001b[0m with correctness \u001b[32m100.00%\u001b[39m.\n",
      "If \u001b[1mc_charge_degree = F, juv_misd_count = 0, juv_other_count = 1, priors_count = (-0.1, 1.0], sex = Male\u001b[0m:\n",
      "\tProtected Subgroup '\u001b[1mAfrican-American\u001b[0m', \u001b[34m1.94%\u001b[39m covered\n",
      "\t\tMake \u001b[1m\u001b[31mjuv_other_count = 0\u001b[39m\u001b[0m with correctness \u001b[32m0.00%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mc_charge_degree = M\u001b[39m, \u001b[31mjuv_other_count = 0\u001b[39m\u001b[0m with correctness \u001b[32m100.00%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mc_charge_degree = M\u001b[39m, \u001b[31mjuv_other_count = 0\u001b[39m, \u001b[31mpriors_count = (1.0, 5.0]\u001b[39m\u001b[0m with correctness \u001b[32m0.00%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mjuv_other_count = 0\u001b[39m, \u001b[31mpriors_count = (1.0, 5.0]\u001b[39m\u001b[0m with correctness \u001b[32m0.00%\u001b[39m.\n",
      "\tProtected Subgroup '\u001b[1mCaucasian\u001b[0m', \u001b[34m2.38%\u001b[39m covered\n",
      "\t\tMake \u001b[1m\u001b[31mjuv_other_count = 0\u001b[39m\u001b[0m with correctness \u001b[32m100.00%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mc_charge_degree = M\u001b[39m, \u001b[31mjuv_other_count = 0\u001b[39m\u001b[0m with correctness \u001b[32m100.00%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mc_charge_degree = M\u001b[39m, \u001b[31mjuv_other_count = 0\u001b[39m, \u001b[31mpriors_count = (1.0, 5.0]\u001b[39m\u001b[0m with correctness \u001b[32m0.00%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mjuv_other_count = 0\u001b[39m, \u001b[31mpriors_count = (1.0, 5.0]\u001b[39m\u001b[0m with correctness \u001b[32m0.00%\u001b[39m.\n",
      "If \u001b[1mc_charge_degree = F, juv_other_count = 1, priors_count = (-0.1, 1.0], sex = Male\u001b[0m:\n",
      "\tProtected Subgroup '\u001b[1mAfrican-American\u001b[0m', \u001b[34m1.94%\u001b[39m covered\n",
      "\t\tMake \u001b[1m\u001b[31mjuv_other_count = 0\u001b[39m\u001b[0m with correctness \u001b[32m0.00%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mc_charge_degree = M\u001b[39m, \u001b[31mjuv_other_count = 0\u001b[39m\u001b[0m with correctness \u001b[32m100.00%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mc_charge_degree = M\u001b[39m, \u001b[31mjuv_other_count = 0\u001b[39m, \u001b[31mpriors_count = (1.0, 5.0]\u001b[39m\u001b[0m with correctness \u001b[32m0.00%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mjuv_other_count = 0\u001b[39m, \u001b[31mpriors_count = (1.0, 5.0]\u001b[39m\u001b[0m with correctness \u001b[32m0.00%\u001b[39m.\n",
      "\tProtected Subgroup '\u001b[1mCaucasian\u001b[0m', \u001b[34m2.38%\u001b[39m covered\n",
      "\t\tMake \u001b[1m\u001b[31mjuv_other_count = 0\u001b[39m\u001b[0m with correctness \u001b[32m100.00%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mc_charge_degree = M\u001b[39m, \u001b[31mjuv_other_count = 0\u001b[39m\u001b[0m with correctness \u001b[32m100.00%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mc_charge_degree = M\u001b[39m, \u001b[31mjuv_other_count = 0\u001b[39m, \u001b[31mpriors_count = (1.0, 5.0]\u001b[39m\u001b[0m with correctness \u001b[32m0.00%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mjuv_other_count = 0\u001b[39m, \u001b[31mpriors_count = (1.0, 5.0]\u001b[39m\u001b[0m with correctness \u001b[32m0.00%\u001b[39m.\n",
      "If \u001b[1mc_charge_degree = F, juv_fel_count = 0, juv_misd_count = 0, juv_other_count = 1, priors_count = (-0.1, 1.0], sex = Male\u001b[0m:\n",
      "\tProtected Subgroup '\u001b[1mAfrican-American\u001b[0m', \u001b[34m1.76%\u001b[39m covered\n",
      "\t\tMake \u001b[1m\u001b[31mjuv_other_count = 0\u001b[39m\u001b[0m with correctness \u001b[32m0.00%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mc_charge_degree = M\u001b[39m, \u001b[31mjuv_other_count = 0\u001b[39m\u001b[0m with correctness \u001b[32m100.00%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mc_charge_degree = M\u001b[39m, \u001b[31mjuv_other_count = 0\u001b[39m, \u001b[31mpriors_count = (1.0, 5.0]\u001b[39m\u001b[0m with correctness \u001b[32m0.00%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mjuv_other_count = 0\u001b[39m, \u001b[31mpriors_count = (1.0, 5.0]\u001b[39m\u001b[0m with correctness \u001b[32m0.00%\u001b[39m.\n",
      "\tProtected Subgroup '\u001b[1mCaucasian\u001b[0m', \u001b[34m2.38%\u001b[39m covered\n",
      "\t\tMake \u001b[1m\u001b[31mjuv_other_count = 0\u001b[39m\u001b[0m with correctness \u001b[32m100.00%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mc_charge_degree = M\u001b[39m, \u001b[31mjuv_other_count = 0\u001b[39m\u001b[0m with correctness \u001b[32m100.00%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mc_charge_degree = M\u001b[39m, \u001b[31mjuv_other_count = 0\u001b[39m, \u001b[31mpriors_count = (1.0, 5.0]\u001b[39m\u001b[0m with correctness \u001b[32m0.00%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mjuv_other_count = 0\u001b[39m, \u001b[31mpriors_count = (1.0, 5.0]\u001b[39m\u001b[0m with correctness \u001b[32m0.00%\u001b[39m.\n",
      "If \u001b[1mc_charge_degree = F, juv_fel_count = 0, juv_other_count = 1, priors_count = (-0.1, 1.0], sex = Male\u001b[0m:\n",
      "\tProtected Subgroup '\u001b[1mAfrican-American\u001b[0m', \u001b[34m1.76%\u001b[39m covered\n",
      "\t\tMake \u001b[1m\u001b[31mjuv_other_count = 0\u001b[39m\u001b[0m with correctness \u001b[32m0.00%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mc_charge_degree = M\u001b[39m, \u001b[31mjuv_other_count = 0\u001b[39m\u001b[0m with correctness \u001b[32m100.00%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mc_charge_degree = M\u001b[39m, \u001b[31mjuv_other_count = 0\u001b[39m, \u001b[31mpriors_count = (1.0, 5.0]\u001b[39m\u001b[0m with correctness \u001b[32m0.00%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mjuv_other_count = 0\u001b[39m, \u001b[31mpriors_count = (1.0, 5.0]\u001b[39m\u001b[0m with correctness \u001b[32m0.00%\u001b[39m.\n",
      "\tProtected Subgroup '\u001b[1mCaucasian\u001b[0m', \u001b[34m2.38%\u001b[39m covered\n",
      "\t\tMake \u001b[1m\u001b[31mjuv_other_count = 0\u001b[39m\u001b[0m with correctness \u001b[32m100.00%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mc_charge_degree = M\u001b[39m, \u001b[31mjuv_other_count = 0\u001b[39m\u001b[0m with correctness \u001b[32m100.00%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mc_charge_degree = M\u001b[39m, \u001b[31mjuv_other_count = 0\u001b[39m, \u001b[31mpriors_count = (1.0, 5.0]\u001b[39m\u001b[0m with correctness \u001b[32m0.00%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mjuv_other_count = 0\u001b[39m, \u001b[31mpriors_count = (1.0, 5.0]\u001b[39m\u001b[0m with correctness \u001b[32m0.00%\u001b[39m.\n",
      "If \u001b[1mc_charge_degree = F, juv_other_count = 1, priors_count = (-0.1, 1.0]\u001b[0m:\n",
      "\tProtected Subgroup '\u001b[1mAfrican-American\u001b[0m', \u001b[34m2.11%\u001b[39m covered\n",
      "\t\tMake \u001b[1m\u001b[31mjuv_other_count = 0\u001b[39m\u001b[0m with correctness \u001b[32m8.33%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mc_charge_degree = M\u001b[39m, \u001b[31mjuv_other_count = 0\u001b[39m\u001b[0m with correctness \u001b[32m100.00%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mjuv_other_count = 0\u001b[39m, \u001b[31mpriors_count = (1.0, 5.0]\u001b[39m\u001b[0m with correctness \u001b[32m0.00%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mc_charge_degree = M\u001b[39m, \u001b[31mjuv_other_count = 0\u001b[39m, \u001b[31mpriors_count = (1.0, 5.0]\u001b[39m\u001b[0m with correctness \u001b[32m0.00%\u001b[39m.\n",
      "\tProtected Subgroup '\u001b[1mCaucasian\u001b[0m', \u001b[34m2.38%\u001b[39m covered\n",
      "\t\tMake \u001b[1m\u001b[31mjuv_other_count = 0\u001b[39m\u001b[0m with correctness \u001b[32m100.00%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mc_charge_degree = M\u001b[39m, \u001b[31mjuv_other_count = 0\u001b[39m\u001b[0m with correctness \u001b[32m100.00%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mjuv_other_count = 0\u001b[39m, \u001b[31mpriors_count = (1.0, 5.0]\u001b[39m\u001b[0m with correctness \u001b[32m0.00%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mc_charge_degree = M\u001b[39m, \u001b[31mjuv_other_count = 0\u001b[39m, \u001b[31mpriors_count = (1.0, 5.0]\u001b[39m\u001b[0m with correctness \u001b[32m0.00%\u001b[39m.\n",
      "If \u001b[1mc_charge_degree = F, juv_misd_count = 0, juv_other_count = 1, priors_count = (-0.1, 1.0]\u001b[0m:\n",
      "\tProtected Subgroup '\u001b[1mAfrican-American\u001b[0m', \u001b[34m2.11%\u001b[39m covered\n",
      "\t\tMake \u001b[1m\u001b[31mjuv_other_count = 0\u001b[39m\u001b[0m with correctness \u001b[32m8.33%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mc_charge_degree = M\u001b[39m, \u001b[31mjuv_other_count = 0\u001b[39m\u001b[0m with correctness \u001b[32m100.00%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mjuv_other_count = 0\u001b[39m, \u001b[31mpriors_count = (1.0, 5.0]\u001b[39m\u001b[0m with correctness \u001b[32m0.00%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mc_charge_degree = M\u001b[39m, \u001b[31mjuv_other_count = 0\u001b[39m, \u001b[31mpriors_count = (1.0, 5.0]\u001b[39m\u001b[0m with correctness \u001b[32m0.00%\u001b[39m.\n",
      "\tProtected Subgroup '\u001b[1mCaucasian\u001b[0m', \u001b[34m2.38%\u001b[39m covered\n",
      "\t\tMake \u001b[1m\u001b[31mjuv_other_count = 0\u001b[39m\u001b[0m with correctness \u001b[32m100.00%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mc_charge_degree = M\u001b[39m, \u001b[31mjuv_other_count = 0\u001b[39m\u001b[0m with correctness \u001b[32m100.00%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mjuv_other_count = 0\u001b[39m, \u001b[31mpriors_count = (1.0, 5.0]\u001b[39m\u001b[0m with correctness \u001b[32m0.00%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mc_charge_degree = M\u001b[39m, \u001b[31mjuv_other_count = 0\u001b[39m, \u001b[31mpriors_count = (1.0, 5.0]\u001b[39m\u001b[0m with correctness \u001b[32m0.00%\u001b[39m.\n",
      "If \u001b[1mc_charge_degree = F, juv_fel_count = 0, juv_misd_count = 0, juv_other_count = 1, priors_count = (-0.1, 1.0]\u001b[0m:\n",
      "\tProtected Subgroup '\u001b[1mAfrican-American\u001b[0m', \u001b[34m1.94%\u001b[39m covered\n",
      "\t\tMake \u001b[1m\u001b[31mjuv_other_count = 0\u001b[39m\u001b[0m with correctness \u001b[32m9.09%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mc_charge_degree = M\u001b[39m, \u001b[31mjuv_other_count = 0\u001b[39m\u001b[0m with correctness \u001b[32m100.00%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mjuv_other_count = 0\u001b[39m, \u001b[31mpriors_count = (1.0, 5.0]\u001b[39m\u001b[0m with correctness \u001b[32m0.00%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mc_charge_degree = M\u001b[39m, \u001b[31mjuv_other_count = 0\u001b[39m, \u001b[31mpriors_count = (1.0, 5.0]\u001b[39m\u001b[0m with correctness \u001b[32m0.00%\u001b[39m.\n",
      "\tProtected Subgroup '\u001b[1mCaucasian\u001b[0m', \u001b[34m2.38%\u001b[39m covered\n",
      "\t\tMake \u001b[1m\u001b[31mjuv_other_count = 0\u001b[39m\u001b[0m with correctness \u001b[32m100.00%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mc_charge_degree = M\u001b[39m, \u001b[31mjuv_other_count = 0\u001b[39m\u001b[0m with correctness \u001b[32m100.00%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mjuv_other_count = 0\u001b[39m, \u001b[31mpriors_count = (1.0, 5.0]\u001b[39m\u001b[0m with correctness \u001b[32m0.00%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mc_charge_degree = M\u001b[39m, \u001b[31mjuv_other_count = 0\u001b[39m, \u001b[31mpriors_count = (1.0, 5.0]\u001b[39m\u001b[0m with correctness \u001b[32m0.00%\u001b[39m.\n",
      "If \u001b[1mc_charge_degree = F, juv_fel_count = 0, juv_other_count = 1, priors_count = (-0.1, 1.0]\u001b[0m:\n",
      "\tProtected Subgroup '\u001b[1mAfrican-American\u001b[0m', \u001b[34m1.94%\u001b[39m covered\n",
      "\t\tMake \u001b[1m\u001b[31mjuv_other_count = 0\u001b[39m\u001b[0m with correctness \u001b[32m9.09%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mc_charge_degree = M\u001b[39m, \u001b[31mjuv_other_count = 0\u001b[39m\u001b[0m with correctness \u001b[32m100.00%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mjuv_other_count = 0\u001b[39m, \u001b[31mpriors_count = (1.0, 5.0]\u001b[39m\u001b[0m with correctness \u001b[32m0.00%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mc_charge_degree = M\u001b[39m, \u001b[31mjuv_other_count = 0\u001b[39m, \u001b[31mpriors_count = (1.0, 5.0]\u001b[39m\u001b[0m with correctness \u001b[32m0.00%\u001b[39m.\n",
      "\tProtected Subgroup '\u001b[1mCaucasian\u001b[0m', \u001b[34m2.38%\u001b[39m covered\n",
      "\t\tMake \u001b[1m\u001b[31mjuv_other_count = 0\u001b[39m\u001b[0m with correctness \u001b[32m100.00%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mc_charge_degree = M\u001b[39m, \u001b[31mjuv_other_count = 0\u001b[39m\u001b[0m with correctness \u001b[32m100.00%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mjuv_other_count = 0\u001b[39m, \u001b[31mpriors_count = (1.0, 5.0]\u001b[39m\u001b[0m with correctness \u001b[32m0.00%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mc_charge_degree = M\u001b[39m, \u001b[31mjuv_other_count = 0\u001b[39m, \u001b[31mpriors_count = (1.0, 5.0]\u001b[39m\u001b[0m with correctness \u001b[32m0.00%\u001b[39m.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "K = 10\n",
    "\n",
    "# keep K ifs with maximum coverage on Females\n",
    "# TODO: the below is currently dataset dependent\n",
    "# rules_by_if_filtered = sorted(rules_by_if.items(), key=lambda e: e[1][\"F\"][0][1], reverse=True)[:K]\n",
    "\n",
    "# keep K ifs with maximum absolute difference in correctness between Males and Females.\n",
    "# TODO: the below is currently dataset dependent\n",
    "rules_by_if_filtered = sorted(rules_by_if.items(), key=lambda e: abs(e[1][\"Caucasian\"][1][0][1]-e[1][\"African-American\"][1][0][1]), reverse=True)[:K]\n",
    "\n",
    "rules_by_if_filtered = dict(rules_by_if_filtered)\n",
    "\n",
    "print(recourse_report_reverse(rules_by_if_filtered))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter rules by different fairness definitions\n",
    "\n",
    "Here, we explore more sophisticated ways of picking the \"top\" K rules and showing only those.\n",
    "\n",
    "We start ranking the rule \"groups\" (all recourses for a single \"if\") by averaging out the costs of all suggested recourses, weighted by the respective correctness. For now, the \"cost\" of a recourse is simply the number of features whose value changes (and the magnitude of the change for numerical features, but here there are none)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_features = X._get_numeric_data().columns.to_list()\n",
    "cate_features = X.select_dtypes(include=['object','category']).columns.to_list()\n",
    "ord_features = []\n",
    "feature_weights = {'sex':100,\t'age_cat':10,\t'race':100,\t'juv_fel_count':1,\t'juv_fel_count':1,\t'juv_other_count':1,\t'priors_count':1,\t'c_charge_degree':1}\n",
    "\n",
    "features_with_binary_cost = cate_features\n",
    "features_with_proportional_cost = num_features\n",
    "\n",
    "\n",
    "comparators = facts.feature_change_builder(\n",
    "    X,\n",
    "    num_cols=features_with_proportional_cost,\n",
    "    cate_cols=features_with_binary_cost,\n",
    "    ord_cols=ord_features,\n",
    "    feature_weights=feature_weights,\n",
    "    num_normalization=False,\n",
    "    #feats_to_normalize = [\"capital-gain\",\"capital-loss\"]\n",
    ")\n",
    "params = ParameterProxy(featureChanges=comparators)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Average Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If \u001b[1mjuv_other_count = 3\u001b[0m:\n",
      "\tProtected Subgroup '\u001b[1mAfrican-American\u001b[0m', \u001b[34m1.23%\u001b[39m covered out of 3173\n",
      "\t\tMake \u001b[1m\u001b[31mjuv_other_count = 0\u001b[39m\u001b[0m with correctness \u001b[32m0.00%\u001b[39m.\n",
      "\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35m0.00\u001b[39m\n",
      "\tProtected Subgroup '\u001b[1mCaucasian\u001b[0m', \u001b[34m2.38%\u001b[39m covered out of 2100\n",
      "\t\tMake \u001b[1m\u001b[31mjuv_other_count = 0\u001b[39m\u001b[0m with correctness \u001b[32m100.00%\u001b[39m.\n",
      "\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35m-1.00\u001b[39m\n",
      "\t\u001b[35mBias against African-American. Unfairness score = 1.0.\u001b[39m\n",
      "If \u001b[1mjuv_fel_count = 0, juv_other_count = 3\u001b[0m:\n",
      "\tProtected Subgroup '\u001b[1mAfrican-American\u001b[0m', \u001b[34m1.23%\u001b[39m covered out of 3173\n",
      "\t\tMake \u001b[1m\u001b[31mjuv_other_count = 0\u001b[39m\u001b[0m with correctness \u001b[32m0.00%\u001b[39m.\n",
      "\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35m0.00\u001b[39m\n",
      "\tProtected Subgroup '\u001b[1mCaucasian\u001b[0m', \u001b[34m2.38%\u001b[39m covered out of 2100\n",
      "\t\tMake \u001b[1m\u001b[31mjuv_other_count = 0\u001b[39m\u001b[0m with correctness \u001b[32m100.00%\u001b[39m.\n",
      "\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35m-1.00\u001b[39m\n",
      "\t\u001b[35mBias against African-American. Unfairness score = 1.0.\u001b[39m\n",
      "If \u001b[1mage_cat = 25 - 45, juv_fel_count = 0, juv_other_count = 0, sex = Female\u001b[0m:\n",
      "\tProtected Subgroup '\u001b[1mAfrican-American\u001b[0m', \u001b[34m3.35%\u001b[39m covered out of 3173\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m\u001b[0m with correctness \u001b[32m0.00%\u001b[39m.\n",
      "\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35m0.00\u001b[39m\n",
      "\tProtected Subgroup '\u001b[1mCaucasian\u001b[0m', \u001b[34m3.57%\u001b[39m covered out of 2100\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m\u001b[0m with correctness \u001b[32m66.67%\u001b[39m.\n",
      "\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35m-0.67\u001b[39m\n",
      "\t\u001b[35mBias against African-American. Unfairness score = 0.67.\u001b[39m\n",
      "If \u001b[1mage_cat = 25 - 45, juv_other_count = 0, sex = Female\u001b[0m:\n",
      "\tProtected Subgroup '\u001b[1mAfrican-American\u001b[0m', \u001b[34m3.35%\u001b[39m covered out of 3173\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m\u001b[0m with correctness \u001b[32m0.00%\u001b[39m.\n",
      "\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35m0.00\u001b[39m\n",
      "\tProtected Subgroup '\u001b[1mCaucasian\u001b[0m', \u001b[34m3.57%\u001b[39m covered out of 2100\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m\u001b[0m with correctness \u001b[32m66.67%\u001b[39m.\n",
      "\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35m-0.67\u001b[39m\n",
      "\t\u001b[35mBias against African-American. Unfairness score = 0.67.\u001b[39m\n",
      "If \u001b[1mage_cat = 25 - 45, juv_fel_count = 0, juv_misd_count = 0, juv_other_count = 0, sex = Female\u001b[0m:\n",
      "\tProtected Subgroup '\u001b[1mAfrican-American\u001b[0m', \u001b[34m2.64%\u001b[39m covered out of 3173\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m\u001b[0m with correctness \u001b[32m0.00%\u001b[39m.\n",
      "\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35m0.00\u001b[39m\n",
      "\tProtected Subgroup '\u001b[1mCaucasian\u001b[0m', \u001b[34m3.57%\u001b[39m covered out of 2100\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m\u001b[0m with correctness \u001b[32m66.67%\u001b[39m.\n",
      "\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35m-0.67\u001b[39m\n",
      "\t\u001b[35mBias against African-American. Unfairness score = 0.67.\u001b[39m\n",
      "If \u001b[1mage_cat = 25 - 45, juv_misd_count = 0, juv_other_count = 0, sex = Female\u001b[0m:\n",
      "\tProtected Subgroup '\u001b[1mAfrican-American\u001b[0m', \u001b[34m2.64%\u001b[39m covered out of 3173\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m\u001b[0m with correctness \u001b[32m0.00%\u001b[39m.\n",
      "\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35m0.00\u001b[39m\n",
      "\tProtected Subgroup '\u001b[1mCaucasian\u001b[0m', \u001b[34m3.57%\u001b[39m covered out of 2100\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m\u001b[0m with correctness \u001b[32m66.67%\u001b[39m.\n",
      "\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35m-0.67\u001b[39m\n",
      "\t\u001b[35mBias against African-American. Unfairness score = 0.67.\u001b[39m\n",
      "If \u001b[1mage_cat = 25 - 45, sex = Female\u001b[0m:\n",
      "\tProtected Subgroup '\u001b[1mAfrican-American\u001b[0m', \u001b[34m3.70%\u001b[39m covered out of 3173\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m\u001b[0m with correctness \u001b[32m4.76%\u001b[39m.\n",
      "\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35m-0.05\u001b[39m\n",
      "\tProtected Subgroup '\u001b[1mCaucasian\u001b[0m', \u001b[34m4.17%\u001b[39m covered out of 2100\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m\u001b[0m with correctness \u001b[32m57.14%\u001b[39m.\n",
      "\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35m-0.57\u001b[39m\n",
      "\t\u001b[35mBias against African-American. Unfairness score = 0.52.\u001b[39m\n",
      "If \u001b[1mage_cat = 25 - 45, juv_fel_count = 0, sex = Female\u001b[0m:\n",
      "\tProtected Subgroup '\u001b[1mAfrican-American\u001b[0m', \u001b[34m3.70%\u001b[39m covered out of 3173\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m\u001b[0m with correctness \u001b[32m4.76%\u001b[39m.\n",
      "\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35m-0.05\u001b[39m\n",
      "\tProtected Subgroup '\u001b[1mCaucasian\u001b[0m', \u001b[34m4.17%\u001b[39m covered out of 2100\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m\u001b[0m with correctness \u001b[32m57.14%\u001b[39m.\n",
      "\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35m-0.57\u001b[39m\n",
      "\t\u001b[35mBias against African-American. Unfairness score = 0.52.\u001b[39m\n",
      "If \u001b[1mage_cat = 25 - 45, juv_misd_count = 0, sex = Female\u001b[0m:\n",
      "\tProtected Subgroup '\u001b[1mAfrican-American\u001b[0m', \u001b[34m2.99%\u001b[39m covered out of 3173\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m\u001b[0m with correctness \u001b[32m5.88%\u001b[39m.\n",
      "\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35m-0.06\u001b[39m\n",
      "\tProtected Subgroup '\u001b[1mCaucasian\u001b[0m', \u001b[34m4.17%\u001b[39m covered out of 2100\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m\u001b[0m with correctness \u001b[32m57.14%\u001b[39m.\n",
      "\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35m-0.57\u001b[39m\n",
      "\t\u001b[35mBias against African-American. Unfairness score = 0.51.\u001b[39m\n",
      "If \u001b[1mage_cat = 25 - 45, juv_fel_count = 0, juv_misd_count = 0, sex = Female\u001b[0m:\n",
      "\tProtected Subgroup '\u001b[1mAfrican-American\u001b[0m', \u001b[34m2.99%\u001b[39m covered out of 3173\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m\u001b[0m with correctness \u001b[32m5.88%\u001b[39m.\n",
      "\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35m-0.06\u001b[39m\n",
      "\tProtected Subgroup '\u001b[1mCaucasian\u001b[0m', \u001b[34m4.17%\u001b[39m covered out of 2100\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m\u001b[0m with correctness \u001b[32m57.14%\u001b[39m.\n",
      "\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35m-0.57\u001b[39m\n",
      "\t\u001b[35mBias against African-American. Unfairness score = 0.51.\u001b[39m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_rules, subgroup_costs = facts.select_rules_subset(\n",
    "    rules_by_if,\n",
    "    metric = \"weighted-average\",\n",
    "    sort_strategy = \"abs-diff-decr\",\n",
    "    top_count = 10\n",
    ")\n",
    "\n",
    "pop_sizes = {sg: (X[\"race\"] == sg).sum() for sg in X[\"race\"].unique()}\n",
    "print(recourse_report_reverse(\n",
    "    top_rules,\n",
    "    population_sizes=pop_sizes,\n",
    "    subgroup_costs=subgroup_costs,\n",
    "    show_subgroup_costs=True\n",
    "))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimum cost above threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If \u001b[1mage_cat = 25 - 45\u001b[0m:\n",
      "\tProtected Subgroup '\u001b[1mAfrican-American\u001b[0m', \u001b[34m61.44%\u001b[39m covered out of 3173\n",
      "\t\t\u001b[31mNo recourses for this subgroup!\n",
      "\u001b[39m\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35minf\u001b[39m\n",
      "\tProtected Subgroup '\u001b[1mCaucasian\u001b[0m', \u001b[34m61.31%\u001b[39m covered out of 2100\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m\u001b[0m with correctness \u001b[32m58.25%\u001b[39m.\n",
      "\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35m1.00\u001b[39m\n",
      "\t\u001b[35mBias against African-American. Unfairness score = inf.\u001b[39m\n",
      "If \u001b[1mage_cat = 25 - 45, sex = Male\u001b[0m:\n",
      "\tProtected Subgroup '\u001b[1mAfrican-American\u001b[0m', \u001b[34m57.75%\u001b[39m covered out of 3173\n",
      "\t\t\u001b[31mNo recourses for this subgroup!\n",
      "\u001b[39m\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35minf\u001b[39m\n",
      "\tProtected Subgroup '\u001b[1mCaucasian\u001b[0m', \u001b[34m57.14%\u001b[39m covered out of 2100\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m\u001b[0m with correctness \u001b[32m58.33%\u001b[39m.\n",
      "\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35m1.00\u001b[39m\n",
      "\t\u001b[35mBias against African-American. Unfairness score = inf.\u001b[39m\n",
      "If \u001b[1mage_cat = 25 - 45, juv_fel_count = 0\u001b[0m:\n",
      "\tProtected Subgroup '\u001b[1mAfrican-American\u001b[0m', \u001b[34m56.69%\u001b[39m covered out of 3173\n",
      "\t\t\u001b[31mNo recourses for this subgroup!\n",
      "\u001b[39m\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35minf\u001b[39m\n",
      "\tProtected Subgroup '\u001b[1mCaucasian\u001b[0m', \u001b[34m60.12%\u001b[39m covered out of 2100\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m\u001b[0m with correctness \u001b[32m59.41%\u001b[39m.\n",
      "\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35m1.00\u001b[39m\n",
      "\t\u001b[35mBias against African-American. Unfairness score = inf.\u001b[39m\n",
      "If \u001b[1mc_charge_degree = F, juv_misd_count = 0, juv_other_count = 0, sex = Male\u001b[0m:\n",
      "\tProtected Subgroup '\u001b[1mAfrican-American\u001b[0m', \u001b[34m55.99%\u001b[39m covered out of 3173\n",
      "\t\t\u001b[31mNo recourses for this subgroup!\n",
      "\u001b[39m\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35minf\u001b[39m\n",
      "\tProtected Subgroup '\u001b[1mCaucasian\u001b[0m', \u001b[34m60.12%\u001b[39m covered out of 2100\n",
      "\t\tMake \u001b[1m\u001b[31mc_charge_degree = M\u001b[39m\u001b[0m with correctness \u001b[32m50.50%\u001b[39m.\n",
      "\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35m1.00\u001b[39m\n",
      "\t\u001b[35mBias against African-American. Unfairness score = inf.\u001b[39m\n",
      "If \u001b[1mage_cat = 25 - 45, juv_other_count = 0\u001b[0m:\n",
      "\tProtected Subgroup '\u001b[1mAfrican-American\u001b[0m', \u001b[34m54.23%\u001b[39m covered out of 3173\n",
      "\t\t\u001b[31mNo recourses for this subgroup!\n",
      "\u001b[39m\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35minf\u001b[39m\n",
      "\tProtected Subgroup '\u001b[1mCaucasian\u001b[0m', \u001b[34m55.95%\u001b[39m covered out of 2100\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m\u001b[0m with correctness \u001b[32m58.51%\u001b[39m.\n",
      "\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35m1.00\u001b[39m\n",
      "\t\u001b[35mBias against African-American. Unfairness score = inf.\u001b[39m\n",
      "If \u001b[1mage_cat = 25 - 45, juv_fel_count = 0, sex = Male\u001b[0m:\n",
      "\tProtected Subgroup '\u001b[1mAfrican-American\u001b[0m', \u001b[34m52.99%\u001b[39m covered out of 3173\n",
      "\t\t\u001b[31mNo recourses for this subgroup!\n",
      "\u001b[39m\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35minf\u001b[39m\n",
      "\tProtected Subgroup '\u001b[1mCaucasian\u001b[0m', \u001b[34m55.95%\u001b[39m covered out of 2100\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m\u001b[0m with correctness \u001b[32m59.57%\u001b[39m.\n",
      "\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35m1.00\u001b[39m\n",
      "\t\u001b[35mBias against African-American. Unfairness score = inf.\u001b[39m\n",
      "If \u001b[1mc_charge_degree = F, juv_fel_count = 0, juv_misd_count = 0, juv_other_count = 0, sex = Male\u001b[0m:\n",
      "\tProtected Subgroup '\u001b[1mAfrican-American\u001b[0m', \u001b[34m52.46%\u001b[39m covered out of 3173\n",
      "\t\t\u001b[31mNo recourses for this subgroup!\n",
      "\u001b[39m\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35minf\u001b[39m\n",
      "\tProtected Subgroup '\u001b[1mCaucasian\u001b[0m', \u001b[34m60.12%\u001b[39m covered out of 2100\n",
      "\t\tMake \u001b[1m\u001b[31mc_charge_degree = M\u001b[39m\u001b[0m with correctness \u001b[32m50.50%\u001b[39m.\n",
      "\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35m1.00\u001b[39m\n",
      "\t\u001b[35mBias against African-American. Unfairness score = inf.\u001b[39m\n",
      "If \u001b[1mage_cat = 25 - 45, juv_other_count = 0, sex = Male\u001b[0m:\n",
      "\tProtected Subgroup '\u001b[1mAfrican-American\u001b[0m', \u001b[34m50.88%\u001b[39m covered out of 3173\n",
      "\t\t\u001b[31mNo recourses for this subgroup!\n",
      "\u001b[39m\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35minf\u001b[39m\n",
      "\tProtected Subgroup '\u001b[1mCaucasian\u001b[0m', \u001b[34m52.38%\u001b[39m covered out of 2100\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m\u001b[0m with correctness \u001b[32m57.95%\u001b[39m.\n",
      "\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35m1.00\u001b[39m\n",
      "\t\u001b[35mBias against African-American. Unfairness score = inf.\u001b[39m\n",
      "If \u001b[1mage_cat = 25 - 45, juv_fel_count = 0, juv_other_count = 0\u001b[0m:\n",
      "\tProtected Subgroup '\u001b[1mAfrican-American\u001b[0m', \u001b[34m50.35%\u001b[39m covered out of 3173\n",
      "\t\t\u001b[31mNo recourses for this subgroup!\n",
      "\u001b[39m\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35minf\u001b[39m\n",
      "\tProtected Subgroup '\u001b[1mCaucasian\u001b[0m', \u001b[34m54.76%\u001b[39m covered out of 2100\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m\u001b[0m with correctness \u001b[32m59.78%\u001b[39m.\n",
      "\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35m1.00\u001b[39m\n",
      "\t\u001b[35mBias against African-American. Unfairness score = inf.\u001b[39m\n",
      "If \u001b[1mage_cat = 25 - 45, c_charge_degree = F\u001b[0m:\n",
      "\tProtected Subgroup '\u001b[1mAfrican-American\u001b[0m', \u001b[34m46.13%\u001b[39m covered out of 3173\n",
      "\t\t\u001b[31mNo recourses for this subgroup!\n",
      "\u001b[39m\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35minf\u001b[39m\n",
      "\tProtected Subgroup '\u001b[1mCaucasian\u001b[0m', \u001b[34m51.79%\u001b[39m covered out of 2100\n",
      "\t\tMake \u001b[1m\u001b[31mc_charge_degree = M\u001b[39m\u001b[0m with correctness \u001b[32m58.62%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m\u001b[0m with correctness \u001b[32m63.22%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m, \u001b[31mc_charge_degree = M\u001b[39m\u001b[0m with correctness \u001b[32m64.37%\u001b[39m.\n",
      "\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35m1.00\u001b[39m\n",
      "\t\u001b[35mBias against African-American. Unfairness score = inf.\u001b[39m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_rules, subgroup_costs = facts.select_rules_subset(\n",
    "    rules_by_if,\n",
    "    metric=\"min-above-thr\",\n",
    "    sort_strategy=\"abs-diff-decr-ignore-forall-subgroups-empty\",\n",
    "    top_count=10,\n",
    "    cor_threshold=0.5,\n",
    "    filter_sequence=[\"remove-below-thr\", \"remove-fair-rules\"],\n",
    ")\n",
    "\n",
    "pop_sizes = {sg: (X[\"race\"] == sg).sum() for sg in X[\"race\"].unique()}\n",
    "print(recourse_report_reverse(\n",
    "    top_rules,\n",
    "    population_sizes = pop_sizes,\n",
    "    subgroup_costs=subgroup_costs,\n",
    "    show_subgroup_costs=True\n",
    "))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean cost above threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If \u001b[1mage_cat = 25 - 45\u001b[0m:\n",
      "\tProtected Subgroup '\u001b[1mAfrican-American\u001b[0m', \u001b[34m61.44%\u001b[39m covered out of 3173\n",
      "\t\t\u001b[31mNo recourses for this subgroup!\n",
      "\u001b[39m\tProtected Subgroup '\u001b[1mCaucasian\u001b[0m', \u001b[34m61.31%\u001b[39m covered out of 2100\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m\u001b[0m with correctness \u001b[32m58.25%\u001b[39m.\n",
      "\t\u001b[35mBias against African-American. Unfairness score = inf.\u001b[39m\n",
      "If \u001b[1mage_cat = 25 - 45, sex = Male\u001b[0m:\n",
      "\tProtected Subgroup '\u001b[1mAfrican-American\u001b[0m', \u001b[34m57.75%\u001b[39m covered out of 3173\n",
      "\t\t\u001b[31mNo recourses for this subgroup!\n",
      "\u001b[39m\tProtected Subgroup '\u001b[1mCaucasian\u001b[0m', \u001b[34m57.14%\u001b[39m covered out of 2100\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m\u001b[0m with correctness \u001b[32m58.33%\u001b[39m.\n",
      "\t\u001b[35mBias against African-American. Unfairness score = inf.\u001b[39m\n",
      "If \u001b[1mage_cat = 25 - 45, juv_fel_count = 0\u001b[0m:\n",
      "\tProtected Subgroup '\u001b[1mAfrican-American\u001b[0m', \u001b[34m56.69%\u001b[39m covered out of 3173\n",
      "\t\t\u001b[31mNo recourses for this subgroup!\n",
      "\u001b[39m\tProtected Subgroup '\u001b[1mCaucasian\u001b[0m', \u001b[34m60.12%\u001b[39m covered out of 2100\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m\u001b[0m with correctness \u001b[32m59.41%\u001b[39m.\n",
      "\t\u001b[35mBias against African-American. Unfairness score = inf.\u001b[39m\n",
      "If \u001b[1mc_charge_degree = F, juv_misd_count = 0, juv_other_count = 0, sex = Male\u001b[0m:\n",
      "\tProtected Subgroup '\u001b[1mAfrican-American\u001b[0m', \u001b[34m55.99%\u001b[39m covered out of 3173\n",
      "\t\t\u001b[31mNo recourses for this subgroup!\n",
      "\u001b[39m\tProtected Subgroup '\u001b[1mCaucasian\u001b[0m', \u001b[34m60.12%\u001b[39m covered out of 2100\n",
      "\t\tMake \u001b[1m\u001b[31mc_charge_degree = M\u001b[39m\u001b[0m with correctness \u001b[32m50.50%\u001b[39m.\n",
      "\t\u001b[35mBias against African-American. Unfairness score = inf.\u001b[39m\n",
      "If \u001b[1mage_cat = 25 - 45, juv_other_count = 0\u001b[0m:\n",
      "\tProtected Subgroup '\u001b[1mAfrican-American\u001b[0m', \u001b[34m54.23%\u001b[39m covered out of 3173\n",
      "\t\t\u001b[31mNo recourses for this subgroup!\n",
      "\u001b[39m\tProtected Subgroup '\u001b[1mCaucasian\u001b[0m', \u001b[34m55.95%\u001b[39m covered out of 2100\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m\u001b[0m with correctness \u001b[32m58.51%\u001b[39m.\n",
      "\t\u001b[35mBias against African-American. Unfairness score = inf.\u001b[39m\n",
      "If \u001b[1mage_cat = 25 - 45, juv_fel_count = 0, sex = Male\u001b[0m:\n",
      "\tProtected Subgroup '\u001b[1mAfrican-American\u001b[0m', \u001b[34m52.99%\u001b[39m covered out of 3173\n",
      "\t\t\u001b[31mNo recourses for this subgroup!\n",
      "\u001b[39m\tProtected Subgroup '\u001b[1mCaucasian\u001b[0m', \u001b[34m55.95%\u001b[39m covered out of 2100\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m\u001b[0m with correctness \u001b[32m59.57%\u001b[39m.\n",
      "\t\u001b[35mBias against African-American. Unfairness score = inf.\u001b[39m\n",
      "If \u001b[1mc_charge_degree = F, juv_fel_count = 0, juv_misd_count = 0, juv_other_count = 0, sex = Male\u001b[0m:\n",
      "\tProtected Subgroup '\u001b[1mAfrican-American\u001b[0m', \u001b[34m52.46%\u001b[39m covered out of 3173\n",
      "\t\t\u001b[31mNo recourses for this subgroup!\n",
      "\u001b[39m\tProtected Subgroup '\u001b[1mCaucasian\u001b[0m', \u001b[34m60.12%\u001b[39m covered out of 2100\n",
      "\t\tMake \u001b[1m\u001b[31mc_charge_degree = M\u001b[39m\u001b[0m with correctness \u001b[32m50.50%\u001b[39m.\n",
      "\t\u001b[35mBias against African-American. Unfairness score = inf.\u001b[39m\n",
      "If \u001b[1mage_cat = 25 - 45, juv_other_count = 0, sex = Male\u001b[0m:\n",
      "\tProtected Subgroup '\u001b[1mAfrican-American\u001b[0m', \u001b[34m50.88%\u001b[39m covered out of 3173\n",
      "\t\t\u001b[31mNo recourses for this subgroup!\n",
      "\u001b[39m\tProtected Subgroup '\u001b[1mCaucasian\u001b[0m', \u001b[34m52.38%\u001b[39m covered out of 2100\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m\u001b[0m with correctness \u001b[32m57.95%\u001b[39m.\n",
      "\t\u001b[35mBias against African-American. Unfairness score = inf.\u001b[39m\n",
      "If \u001b[1mage_cat = 25 - 45, juv_fel_count = 0, juv_other_count = 0\u001b[0m:\n",
      "\tProtected Subgroup '\u001b[1mAfrican-American\u001b[0m', \u001b[34m50.35%\u001b[39m covered out of 3173\n",
      "\t\t\u001b[31mNo recourses for this subgroup!\n",
      "\u001b[39m\tProtected Subgroup '\u001b[1mCaucasian\u001b[0m', \u001b[34m54.76%\u001b[39m covered out of 2100\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m\u001b[0m with correctness \u001b[32m59.78%\u001b[39m.\n",
      "\t\u001b[35mBias against African-American. Unfairness score = inf.\u001b[39m\n",
      "If \u001b[1mage_cat = 25 - 45, c_charge_degree = F\u001b[0m:\n",
      "\tProtected Subgroup '\u001b[1mAfrican-American\u001b[0m', \u001b[34m46.13%\u001b[39m covered out of 3173\n",
      "\t\t\u001b[31mNo recourses for this subgroup!\n",
      "\u001b[39m\tProtected Subgroup '\u001b[1mCaucasian\u001b[0m', \u001b[34m51.79%\u001b[39m covered out of 2100\n",
      "\t\tMake \u001b[1m\u001b[31mc_charge_degree = M\u001b[39m\u001b[0m with correctness \u001b[32m58.62%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m\u001b[0m with correctness \u001b[32m63.22%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m, \u001b[31mc_charge_degree = M\u001b[39m\u001b[0m with correctness \u001b[32m64.37%\u001b[39m.\n",
      "\t\u001b[35mBias against African-American. Unfairness score = inf.\u001b[39m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_rules, subgroup_costs = facts.select_rules_subset(\n",
    "    rules_by_if,\n",
    "    metric=\"mean-above-thr\",\n",
    "    sort_strategy=\"abs-diff-decr-ignore-forall-subgroups-empty\",\n",
    "    top_count=10,\n",
    "    cor_threshold=0.5,\n",
    "    filter_sequence=[\"remove-below-thr\", \"remove-fair-rules\"]\n",
    ")\n",
    "\n",
    "pop_sizes = {sg: (X[\"race\"] == sg).sum() for sg in X[\"race\"].unique()}\n",
    "print(recourse_report_reverse(\n",
    "    top_rules,\n",
    "    population_sizes = pop_sizes,\n",
    "    subgroup_costs=subgroup_costs\n",
    "))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number above threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If \u001b[1mage_cat = 10-25, c_charge_degree = F, juv_other_count = 1, sex = Male\u001b[0m:\n",
      "\tProtected Subgroup '\u001b[1mAfrican-American\u001b[0m', \u001b[34m4.05%\u001b[39m covered out of 3173\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m, \u001b[31mjuv_other_count = 0\u001b[39m\u001b[0m with correctness \u001b[32m86.96%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m, \u001b[31mc_charge_degree = M\u001b[39m, \u001b[31mjuv_other_count = 0\u001b[39m\u001b[0m with correctness \u001b[32m86.96%\u001b[39m.\n",
      "\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35m-2.00\u001b[39m\n",
      "\tProtected Subgroup '\u001b[1mCaucasian\u001b[0m', \u001b[34m4.76%\u001b[39m covered out of 2100\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = 25 - 45\u001b[39m, \u001b[31mjuv_other_count = 0\u001b[39m\u001b[0m with correctness \u001b[32m50.00%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = 25 - 45\u001b[39m, \u001b[31mc_charge_degree = M\u001b[39m, \u001b[31mjuv_other_count = 0\u001b[39m\u001b[0m with correctness \u001b[32m87.50%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m, \u001b[31mjuv_other_count = 0\u001b[39m\u001b[0m with correctness \u001b[32m87.50%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m, \u001b[31mc_charge_degree = M\u001b[39m, \u001b[31mjuv_other_count = 0\u001b[39m\u001b[0m with correctness \u001b[32m87.50%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mc_charge_degree = M\u001b[39m, \u001b[31mjuv_other_count = 0\u001b[39m\u001b[0m with correctness \u001b[32m50.00%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mjuv_other_count = 0\u001b[39m\u001b[0m with correctness \u001b[32m50.00%\u001b[39m.\n",
      "\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35m-6.00\u001b[39m\n",
      "\t\u001b[35mBias against African-American. Unfairness score = 4.\u001b[39m\n",
      "If \u001b[1mage_cat = 25 - 45, c_charge_degree = F\u001b[0m:\n",
      "\tProtected Subgroup '\u001b[1mAfrican-American\u001b[0m', \u001b[34m46.13%\u001b[39m covered out of 3173\n",
      "\t\t\u001b[31mNo recourses for this subgroup!\n",
      "\u001b[39m\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35m0.00\u001b[39m\n",
      "\tProtected Subgroup '\u001b[1mCaucasian\u001b[0m', \u001b[34m51.79%\u001b[39m covered out of 2100\n",
      "\t\tMake \u001b[1m\u001b[31mc_charge_degree = M\u001b[39m\u001b[0m with correctness \u001b[32m58.62%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m\u001b[0m with correctness \u001b[32m63.22%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m, \u001b[31mc_charge_degree = M\u001b[39m\u001b[0m with correctness \u001b[32m64.37%\u001b[39m.\n",
      "\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35m-3.00\u001b[39m\n",
      "\t\u001b[35mBias against African-American. Unfairness score = 3.\u001b[39m\n",
      "If \u001b[1mage_cat = 25 - 45, c_charge_degree = F, sex = Male\u001b[0m:\n",
      "\tProtected Subgroup '\u001b[1mAfrican-American\u001b[0m', \u001b[34m42.96%\u001b[39m covered out of 3173\n",
      "\t\t\u001b[31mNo recourses for this subgroup!\n",
      "\u001b[39m\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35m0.00\u001b[39m\n",
      "\tProtected Subgroup '\u001b[1mCaucasian\u001b[0m', \u001b[34m48.81%\u001b[39m covered out of 2100\n",
      "\t\tMake \u001b[1m\u001b[31mc_charge_degree = M\u001b[39m\u001b[0m with correctness \u001b[32m62.20%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m\u001b[0m with correctness \u001b[32m64.63%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m, \u001b[31mc_charge_degree = M\u001b[39m\u001b[0m with correctness \u001b[32m64.63%\u001b[39m.\n",
      "\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35m-3.00\u001b[39m\n",
      "\t\u001b[35mBias against African-American. Unfairness score = 3.\u001b[39m\n",
      "If \u001b[1mage_cat = 25 - 45, c_charge_degree = F, juv_fel_count = 0\u001b[0m:\n",
      "\tProtected Subgroup '\u001b[1mAfrican-American\u001b[0m', \u001b[34m41.90%\u001b[39m covered out of 3173\n",
      "\t\t\u001b[31mNo recourses for this subgroup!\n",
      "\u001b[39m\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35m0.00\u001b[39m\n",
      "\tProtected Subgroup '\u001b[1mCaucasian\u001b[0m', \u001b[34m51.19%\u001b[39m covered out of 2100\n",
      "\t\tMake \u001b[1m\u001b[31mc_charge_degree = M\u001b[39m\u001b[0m with correctness \u001b[32m59.30%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m\u001b[0m with correctness \u001b[32m63.95%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m, \u001b[31mc_charge_degree = M\u001b[39m\u001b[0m with correctness \u001b[32m65.12%\u001b[39m.\n",
      "\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35m-3.00\u001b[39m\n",
      "\t\u001b[35mBias against African-American. Unfairness score = 3.\u001b[39m\n",
      "If \u001b[1mage_cat = 25 - 45, c_charge_degree = F, juv_other_count = 0\u001b[0m:\n",
      "\tProtected Subgroup '\u001b[1mAfrican-American\u001b[0m', \u001b[34m41.73%\u001b[39m covered out of 3173\n",
      "\t\t\u001b[31mNo recourses for this subgroup!\n",
      "\u001b[39m\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35m0.00\u001b[39m\n",
      "\tProtected Subgroup '\u001b[1mCaucasian\u001b[0m', \u001b[34m49.40%\u001b[39m covered out of 2100\n",
      "\t\tMake \u001b[1m\u001b[31mc_charge_degree = M\u001b[39m\u001b[0m with correctness \u001b[32m61.45%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m\u001b[0m with correctness \u001b[32m63.86%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m, \u001b[31mc_charge_degree = M\u001b[39m\u001b[0m with correctness \u001b[32m65.06%\u001b[39m.\n",
      "\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35m-3.00\u001b[39m\n",
      "\t\u001b[35mBias against African-American. Unfairness score = 3.\u001b[39m\n",
      "If \u001b[1mage_cat = 25 - 45, c_charge_degree = F, juv_other_count = 0, sex = Male\u001b[0m:\n",
      "\tProtected Subgroup '\u001b[1mAfrican-American\u001b[0m', \u001b[34m38.73%\u001b[39m covered out of 3173\n",
      "\t\t\u001b[31mNo recourses for this subgroup!\n",
      "\u001b[39m\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35m0.00\u001b[39m\n",
      "\tProtected Subgroup '\u001b[1mCaucasian\u001b[0m', \u001b[34m47.02%\u001b[39m covered out of 2100\n",
      "\t\tMake \u001b[1m\u001b[31mc_charge_degree = M\u001b[39m\u001b[0m with correctness \u001b[32m64.56%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m\u001b[0m with correctness \u001b[32m64.56%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m, \u001b[31mc_charge_degree = M\u001b[39m\u001b[0m with correctness \u001b[32m64.56%\u001b[39m.\n",
      "\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35m-3.00\u001b[39m\n",
      "\t\u001b[35mBias against African-American. Unfairness score = 3.\u001b[39m\n",
      "If \u001b[1mage_cat = 25 - 45, c_charge_degree = F, juv_fel_count = 0, sex = Male\u001b[0m:\n",
      "\tProtected Subgroup '\u001b[1mAfrican-American\u001b[0m', \u001b[34m38.73%\u001b[39m covered out of 3173\n",
      "\t\t\u001b[31mNo recourses for this subgroup!\n",
      "\u001b[39m\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35m0.00\u001b[39m\n",
      "\tProtected Subgroup '\u001b[1mCaucasian\u001b[0m', \u001b[34m48.21%\u001b[39m covered out of 2100\n",
      "\t\tMake \u001b[1m\u001b[31mc_charge_degree = M\u001b[39m\u001b[0m with correctness \u001b[32m62.96%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m\u001b[0m with correctness \u001b[32m65.43%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m, \u001b[31mc_charge_degree = M\u001b[39m\u001b[0m with correctness \u001b[32m65.43%\u001b[39m.\n",
      "\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35m-3.00\u001b[39m\n",
      "\t\u001b[35mBias against African-American. Unfairness score = 3.\u001b[39m\n",
      "If \u001b[1mage_cat = 25 - 45, c_charge_degree = F, juv_fel_count = 0, juv_other_count = 0\u001b[0m:\n",
      "\tProtected Subgroup '\u001b[1mAfrican-American\u001b[0m', \u001b[34m38.20%\u001b[39m covered out of 3173\n",
      "\t\t\u001b[31mNo recourses for this subgroup!\n",
      "\u001b[39m\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35m0.00\u001b[39m\n",
      "\tProtected Subgroup '\u001b[1mCaucasian\u001b[0m', \u001b[34m48.81%\u001b[39m covered out of 2100\n",
      "\t\tMake \u001b[1m\u001b[31mc_charge_degree = M\u001b[39m\u001b[0m with correctness \u001b[32m62.20%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m\u001b[0m with correctness \u001b[32m64.63%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m, \u001b[31mc_charge_degree = M\u001b[39m\u001b[0m with correctness \u001b[32m65.85%\u001b[39m.\n",
      "\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35m-3.00\u001b[39m\n",
      "\t\u001b[35mBias against African-American. Unfairness score = 3.\u001b[39m\n",
      "If \u001b[1mage_cat = 25 - 45, c_charge_degree = F, juv_fel_count = 0, juv_other_count = 0, sex = Male\u001b[0m:\n",
      "\tProtected Subgroup '\u001b[1mAfrican-American\u001b[0m', \u001b[34m35.21%\u001b[39m covered out of 3173\n",
      "\t\t\u001b[31mNo recourses for this subgroup!\n",
      "\u001b[39m\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35m0.00\u001b[39m\n",
      "\tProtected Subgroup '\u001b[1mCaucasian\u001b[0m', \u001b[34m46.43%\u001b[39m covered out of 2100\n",
      "\t\tMake \u001b[1m\u001b[31mc_charge_degree = M\u001b[39m\u001b[0m with correctness \u001b[32m65.38%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m\u001b[0m with correctness \u001b[32m65.38%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m, \u001b[31mc_charge_degree = M\u001b[39m\u001b[0m with correctness \u001b[32m65.38%\u001b[39m.\n",
      "\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35m-3.00\u001b[39m\n",
      "\t\u001b[35mBias against African-American. Unfairness score = 3.\u001b[39m\n",
      "If \u001b[1mage_cat = 25 - 45, c_charge_degree = F, juv_other_count = 1\u001b[0m:\n",
      "\tProtected Subgroup '\u001b[1mAfrican-American\u001b[0m', \u001b[34m3.35%\u001b[39m covered out of 3173\n",
      "\t\t\u001b[31mNo recourses for this subgroup!\n",
      "\u001b[39m\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35m0.00\u001b[39m\n",
      "\tProtected Subgroup '\u001b[1mCaucasian\u001b[0m', \u001b[34m1.79%\u001b[39m covered out of 2100\n",
      "\t\tMake \u001b[1m\u001b[31mc_charge_degree = M\u001b[39m, \u001b[31mjuv_other_count = 0\u001b[39m\u001b[0m with correctness \u001b[32m66.67%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m, \u001b[31mjuv_other_count = 0\u001b[39m\u001b[0m with correctness \u001b[32m66.67%\u001b[39m.\n",
      "\t\tMake \u001b[1m\u001b[31mage_cat = Greater than 45\u001b[39m, \u001b[31mc_charge_degree = M\u001b[39m, \u001b[31mjuv_other_count = 0\u001b[39m\u001b[0m with correctness \u001b[32m66.67%\u001b[39m.\n",
      "\t\t\u001b[1mAggregate cost\u001b[0m of the above recourses = \u001b[35m-3.00\u001b[39m\n",
      "\t\u001b[35mBias against African-American. Unfairness score = 3.\u001b[39m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_rules, subgroup_costs = facts.select_rules_subset(\n",
    "    rules_by_if,\n",
    "    metric=\"num-above-thr\",\n",
    "    sort_strategy=\"abs-diff-decr-ignore-forall-subgroups-empty\",\n",
    "    top_count=10,\n",
    "    cor_threshold=0.5,\n",
    "    filter_sequence=[\"remove-below-thr\"]\n",
    ")\n",
    "\n",
    "pop_sizes = {sg: (X[\"race\"] == sg).sum() for sg in X[\"race\"].unique()}\n",
    "print(recourse_report_reverse(\n",
    "    top_rules,\n",
    "    population_sizes = pop_sizes,\n",
    "    subgroup_costs=subgroup_costs,\n",
    "    show_subgroup_costs=True\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1011/1011 [00:23<00:00, 42.49it/s]\n"
     ]
    }
   ],
   "source": [
    "rules_with_cumulative = facts.cum_corr_costs_all(rules_by_if, X_test, model, sensitive_attribute=\"race\", params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_rules_by_if(\"rulesCompas_cumulative.data\", rules_with_cumulative)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "362842c44ed7d38e9c92d488ead4445e1c745f35479a5810177dacafb6456c39"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
